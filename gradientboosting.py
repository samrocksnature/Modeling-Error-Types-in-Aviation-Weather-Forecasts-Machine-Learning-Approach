# -*- coding: utf-8 -*-
"""gradientboosting

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17zLYDScptFvRXKDV10Gs9jE1hVnIMZc-
"""



import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import numpy as np

# Load the data
data = pd.read_csv('/content/merged_metar_taf.csv')

# Define the list of features and target variable (remove Ceiling variables)
features = ['Ceiling_TAF (ft)', 'Visibility_TAF (km)', 'Altitude (ft)', 'Latitude (degrees)', 'Longitude (degrees)', 'Distance (km)',
            'Koppen Climate Classification']
target = 'Error_type'

# Fix non-numeric issues
data['Visibility_METAR (km)'] = data['Visibility_METAR (km)'].replace({'10+': 10, '6+': 6}, regex=True).astype(float)
data['Visibility_TAF (km)'] = data['Visibility_TAF (km)'].replace({'6+': 6}, regex=True).astype(float)
data['Longitude (degrees)'] = data['Longitude (degrees)'].str.extract(r'([-0-9.]+)').astype(float)
data['Weather_TAF'] = data['Weather_TAF'].astype('category').cat.codes
data['Koppen Climate Classification'] = data['Koppen Climate Classification'].astype('category').cat.codes
error_mapping = {
    'Correct - No error (0)': 0,
    'Fail to Detect - Type II (2) error': 2,
    'False Alarm - Type I (1) error': 1
}
data['Error_type'] = data['Error_type'].map(error_mapping)
data[features + [target]] = data[features + [target]].apply(pd.to_numeric, errors='coerce')
data = data.dropna()

# Ensure the dataset is not empty
if data.empty:
    raise ValueError("The dataset is empty after filtering. Please check the input data.")

# Splitting the data
X = data[features].values
y = data[target].values

# Scale features for uniformity
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Check class distribution
unique, counts = np.unique(y, return_counts=True)
print("Class Distribution:", dict(zip(unique, counts)))

# Apply train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=566033, stratify=y)

# Apply SMOTE to balance classes
smote = SMOTE(random_state=566033)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Compute class weights
class_weights = {0: 1, 1: len(y) / (3 * counts[1]), 2: len(y) / (3 * counts[2])}  # Example weights

# Create sample weights for the training data
sample_weights = np.array([class_weights[c] for c in y_resampled])

# Fit Gradient Boosting Classifier with sample weights
gbmclass_params = {
    'n_estimators': 200,  # Reduced number of trees
    'max_depth': 3,       # Shallow trees to prevent overfitting
    'learning_rate': 0.2, # Faster learning to regularize
    'subsample': 0.8,     # Subsample data to introduce randomness
    'min_samples_split': 5, # Minimum samples required to split a node
    'min_samples_leaf': 2   # Minimum samples required in a leaf
}
gb_mclass = GradientBoostingClassifier(**gbmclass_params)
gb_mclass.fit(X_resampled, y_resampled, sample_weight=sample_weights)

# Evaluate on test set
y_pred = gb_mclass.predict(X_test)

# Display metrics
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=4))

# Cross-validation for more robust evaluation
cv_scores = cross_val_score(gb_mclass, X_resampled, y_resampled, cv=5, scoring='accuracy')
print(f"\nCross-Validation Accuracy (mean ± std): {cv_scores.mean():.2f} ± {cv_scores.std():.2f}")

# Displaying Variable Importance
var_names = pd.DataFrame(features, columns=['var_name'])
loss_reduction = pd.DataFrame(gb_mclass.feature_importances_, columns=['loss_reduction'])
var_importance = pd.concat([var_names, loss_reduction], axis=1)
print("\nVariable Importance:\n", var_importance.sort_values("loss_reduction", axis=0, ascending=False))